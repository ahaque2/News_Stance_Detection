{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 114
    },
    "colab_type": "code",
    "id": "hXsIKphWoQtm",
    "outputId": "5cb9873c-50e7-43d0-d495-118252746a78"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import pandas as pd\n",
    "import html\n",
    "import re\n",
    "import string\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.summarization import summarize\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from nltk.corpus import stopwords\n",
    "from gensim.summarization import summarize\n",
    "from csv import DictReader\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# Keras Import Statements\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.text import one_hot\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten, Embedding, Input, Lambda\n",
    "from keras.layers.wrappers import Bidirectional\n",
    "from keras.wrappers.scikit_learn import KerasRegressor\n",
    "from keras.optimizers import SGD, Adagrad\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.pooling import MaxPooling1D, GlobalMaxPooling1D, AveragePooling1D\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.layers.recurrent import LSTM, GRU\n",
    "from keras.layers import concatenate,dot\n",
    "from keras.preprocessing import sequence\n",
    "import keras.backend as K\n",
    "from IPython.display import SVG\n",
    "from keras.utils.vis_utils import model_to_dot\n",
    "from keras.utils import plot_model\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Access google drive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 122
    },
    "colab_type": "code",
    "id": "SphKjlPBoUfW",
    "outputId": "29ff937a-761a-4a82-9c9e-3ce11965c0c8"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7Z1c2fqHobbQ"
   },
   "outputs": [],
   "source": [
    "class DataSet():\n",
    "    def __init__(self, name=\"train\", path=\"fnc-1\"):\n",
    "        self.path = path\n",
    "\n",
    "        print(\"Reading dataset\")\n",
    "        bodies = name+\"_bodies.csv\"\n",
    "        stances = name+\"_stances.csv\"\n",
    "\n",
    "        self.stances = self.read(stances)\n",
    "        articles = self.read(bodies)\n",
    "        self.articles = dict()\n",
    "\n",
    "        #make the body ID an integer value\n",
    "        for s in self.stances:\n",
    "            s['Body ID'] = int(s['Body ID'])\n",
    "\n",
    "        #copy all bodies into a dictionary\n",
    "        for article in articles:\n",
    "            self.articles[int(article['Body ID'])] = article['articleBody']\n",
    "\n",
    "        print(\"Total stances: \" + str(len(self.stances)))\n",
    "        print(\"Total bodies: \" + str(len(self.articles)))\n",
    "\n",
    "\n",
    "\n",
    "    def read(self,filename):\n",
    "        rows = []\n",
    "        with open(self.path + \"/\" + filename, \"r\", encoding='utf-8') as table:\n",
    "            r = DictReader(table)\n",
    "\n",
    "            for line in r:\n",
    "                rows.append(line)\n",
    "        return rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 119
    },
    "colab_type": "code",
    "id": "otzX4WQwpyoH",
    "outputId": "7e4323b8-a486-43c1-ce17-ab3bbe0eb1a2"
   },
   "outputs": [],
   "source": [
    "train_dataset = DataSet(name=\"train\",path = \"drive/My Drive/Stance_detection/data/\")\n",
    "test_dataset = DataSet(name=\"test\",path = \"drive/My Drive/Stance_detection/data/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_9V9Ynv2p_-L"
   },
   "outputs": [],
   "source": [
    "global_map = dict()\n",
    "SUMMARY_LEN = 100\n",
    "\n",
    "#Variables for Preprocessing\n",
    "do_summary = True\n",
    "head_stop,head_summary = True, False\n",
    "body_stop,body_summary = True, False\n",
    "\n",
    "# Embedding Dimension\n",
    "EMBEDDING_DIM = 300\n",
    "\n",
    "# Head and body max\n",
    "max_head = 30\n",
    "max_body = 100"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function to Preprocess the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QguENAe4qhcH"
   },
   "outputs": [],
   "source": [
    "# Mean Layer\n",
    "def adder(x):\n",
    "    x = K.mean(x, axis=1)\n",
    "    # x = K.reshape(x,(K.shape(x)[0],K.shape(x)[-1]))\n",
    "    return x\n",
    "\n",
    "def adder_output(input_shape):\n",
    "    shape = list(input_shape)\n",
    "    assert len(shape) == 3  # only valid for 3D tensors\n",
    "    shape = (shape[0],shape[2])\n",
    "    return tuple(shape)\n",
    "\n",
    "def preprocess(text,stop,do_summ):\n",
    "    g_text = text\n",
    "    if g_text in global_map :\n",
    "        return global_map[g_text]\n",
    "    \n",
    "    if do_summ:\n",
    "        temp = re.sub(r'[.]+',\"\\n\",text)\n",
    "        if len(temp.split()) > SUMMARY_LEN:\n",
    "            text = summarize(temp,word_count = SUMMARY_LEN)\n",
    "              \n",
    "    text = html.unescape(text)\n",
    "    text = text.replace(\"\\\\n\",\" \")\n",
    "    text = text.replace(\"_NEG\",\"\")\n",
    "    text = text.replace(\"_NEGFIRST\", \"\")\n",
    "    text = re.sub(r\"#\", \"\", text)\n",
    "    text = re.sub(r\"\\*\", \"\", text)\n",
    "    text = re.sub(r\"\\'s\", \"\", text)\n",
    "    text = re.sub(r\"\\'m\", \" am\", text)\n",
    "    text = re.sub(r\"\\'ve\", \" have\", text)\n",
    "    text = re.sub(r\"n\\'t\", \" not\", text)\n",
    "    text = re.sub(r\"\\'re\", \" are\", text)\n",
    "    text = re.sub(r\"\\'d\", \" would\", text)\n",
    "    text = re.sub(r\"\\'ll\", \" will\", text)\n",
    "    text = re.sub(r\",\", \"\", text)\n",
    "    text = re.sub(r\"!\", \" !\", text)\n",
    "    text = re.sub(r\"\\(\", \"\", text)\n",
    "    text = re.sub(r\"\\)\", \"\", text)\n",
    "    text = re.sub(r\"\\?\", \" ?\", text)\n",
    "    text = re.sub(r'[^\\x00-\\x7F]',' ', text)\n",
    "    text = re.sub(r'[^\\w\\s]',' ',text)\n",
    "    text = re.sub(\"\\d+\", \" \", text)\n",
    "    text = re.sub(r\"\\s{2,}\", \" \", text)\n",
    "    text = text.rstrip(',|.|;|:|\\'|\\\"')\n",
    "    text = text.lstrip('\\'|\\\"')\n",
    "    if stop:\n",
    "        temp = remove_stopwords(text.strip().lower())\n",
    "        global_map[g_text] = temp\n",
    "    else:\n",
    "        temp = (text.strip().lower())\n",
    "        global_map[g_text] = temp\n",
    "    return global_map[g_text]\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    temp = stopwords.words('english')\n",
    "    split_text = \\\n",
    "    [word for word in text.split()\n",
    "        if word not in temp]\n",
    "    return \" \".join(split_text)\n",
    "\n",
    "def create_total_text(dataset,isStance,total_text):\n",
    "    if isStance:\n",
    "        for stance in dataset.stances:\n",
    "            total_text.append(preprocess(stance['Headline'],head_stop,head_summary))\n",
    "    else:\n",
    "        for article_id in dataset.articles:\n",
    "            total_text.append(preprocess(dataset.articles[article_id],body_stop,body_summary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "l-dCTzmkqi54",
    "outputId": "d522feff-9951-4818-fbde-3cda083a5776"
   },
   "outputs": [],
   "source": [
    "# Total Dataset \n",
    "total_text = list()\n",
    "create_total_text(train_dataset,True,total_text)\n",
    "print(len(total_text)) #sanity check\n",
    "create_total_text(train_dataset,False,total_text)\n",
    "print(len(total_text)) #sanity check\n",
    "create_total_text(test_dataset,True,total_text)\n",
    "print(len(total_text)) #sanity check\n",
    "create_total_text(test_dataset,False,total_text)\n",
    "print(len(total_text)) #sanity check"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fiting a tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "qZ5pgknMqk_e",
    "outputId": "2337e5d0-37f4-4f69-ca47-b29494c162d4"
   },
   "outputs": [],
   "source": [
    "t = Tokenizer()\n",
    "t.fit_on_texts(total_text)\n",
    "word_index = t.word_index\n",
    "print(t.document_count)\n",
    "vocab_size = len(t.word_counts)\n",
    "print(vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading work embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "GMRJgSd_qyXb",
    "outputId": "410e3c92-4bdf-4a17-c756-12b6ca7120fb"
   },
   "outputs": [],
   "source": [
    "temp_name = 'embedding_matrix_w2v'\n",
    "create_w2vec = True\n",
    "if do_summary:\n",
    "    temp_name+=\"_summary.npy\"\n",
    "else:\n",
    "    temp_name+=\"_no_summary.npy\"\n",
    "if create_w2vec:\n",
    "    \n",
    "    GLOVE_DIR = \"drive/My Drive/Stance_detection/GoogleNews-vectors-negative300.txt\"\n",
    "    embeddings_index = {}\n",
    "    f = open(GLOVE_DIR)\n",
    "    for line in f:\n",
    "        values = line.split()\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n",
    "    f.close()\n",
    "    print ('Read Word2Vec and Made Dict')\n",
    "\n",
    "    embedding_matrix = np.zeros((vocab_size + 1, EMBEDDING_DIM))\n",
    "    number_found =0\n",
    "    number_not_found = 0\n",
    "    embedding_matrix = np.zeros((len(word_index) + 1, EMBEDDING_DIM))\n",
    "    for word, i in word_index.items():\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            # words not found in embedding index will be all-zeros.\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "            number_found+=1\n",
    "        else:\n",
    "            #print (word)\n",
    "            number_not_found+=1\n",
    "\n",
    "    print(number_found)\n",
    "    print(number_not_found)\n",
    "    np.save(temp_name,embedding_matrix)\n",
    "else:\n",
    "    embedding_matrix = np.load(temp_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating final dataset to be used in dnn models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TYCJHDQPq7LD"
   },
   "outputs": [],
   "source": [
    "def create_dataset(data,ids,isTest,t,max_head,max_body):\n",
    "    \n",
    "    from keras.utils.np_utils import to_categorical\n",
    "    head = list()\n",
    "    body = list()\n",
    "    y = list()\n",
    "    NUM_CLASSES = 4\n",
    "    category_dict = {'unrelated': 0 , 'agree':1, 'disagree':2, 'discuss':3}\n",
    "    for stance in data.stances:\n",
    "        if(isTest):\n",
    "            head.append(preprocess(stance['Headline'],head_stop,head_summary))\n",
    "            body.append(preprocess(data.articles[int(stance['Body ID'])],body_stop,body_summary))\n",
    "            y.append(category_dict[stance['Stance']])\n",
    "            continue\n",
    "        \n",
    "        if stance['Body ID'] not in ids:\n",
    "            continue\n",
    "        \n",
    "        head.append(preprocess(stance['Headline'],head_stop,head_summary))\n",
    "        body.append(preprocess(data.articles[int(stance['Body ID'])],body_stop,body_summary))\n",
    "        y.append(category_dict[stance['Stance']])\n",
    "    \n",
    "    head = t.texts_to_sequences(head)\n",
    "    body = t.texts_to_sequences(body)\n",
    "    head = pad_sequences(head,maxlen = max_head,padding = 'post')\n",
    "    body = pad_sequences(body,maxlen = max_body,padding = 'post')\n",
    "    y_cat = np.zeros((len(y),NUM_CLASSES))\n",
    "    y_cat = to_categorical(y, num_classes=NUM_CLASSES)\n",
    "    return head,body,y_cat\n",
    "\n",
    "def create_labels(data):\n",
    "\n",
    "    from keras.utils.np_utils import to_categorical\n",
    "    category_dict = {'unrelated': 0 , 'agree':1, 'disagree':2, 'discuss':3}\n",
    "    y = list()\n",
    "    NUM_CLASSES = 4\n",
    "    for stance in data.stances:\n",
    "        y.append(category_dict[stance['Stance']])\n",
    "\n",
    "    y_cat = np.zeros((len(y),NUM_CLASSES))\n",
    "    y_cat = to_categorical(y, num_classes=NUM_CLASSES)\n",
    "    return y_cat\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hfOaErfAsTjj"
   },
   "outputs": [],
   "source": [
    "(train_text,train_body,y_train) = create_dataset(train_dataset,None,True,t,max_head,max_body)\n",
    "(test_text,test_body,y_test) = create_dataset(test_dataset,None,True,t,max_head,max_body)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conditional encoding model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "m_N6-dlvtMAo",
    "outputId": "75e4db6a-159f-4d3a-920f-88fe0c5164c3"
   },
   "outputs": [],
   "source": [
    "head_input = Input(shape=(max_head,), dtype='int32', name='head_input')\n",
    "body_input = Input(shape=(max_body,), dtype='int32', name='body_input')\n",
    "shared_embed = Embedding(len(word_index) + 1,EMBEDDING_DIM,weights=[embedding_matrix],trainable=False)\n",
    "head_embed = shared_embed(head_input)\n",
    "body_embed = shared_embed(body_input)\n",
    "\n",
    "body_lstm,state_h,state_c,x,y = Bidirectional(LSTM(100,dropout=0.2, recurrent_dropout=0.2, name='body_lstm',return_state=True))(body_embed)\n",
    "encoded_states = [state_h,state_c,x,y]\n",
    "head_lstm = Bidirectional(LSTM(100,dropout=0.2, recurrent_dropout=0.2, name='head_lstm'))(head_embed,initial_state=encoded_states)\n",
    "dense = Dense(100,activation='relu')(head_lstm)\n",
    "dense = Dropout(0.3)(dense)\n",
    "dense = Dense(100,activation='relu')(dense)\n",
    "dense = Dropout(0.3)(dense)\n",
    "dense = Dense(100,activation='relu')(dense)\n",
    "dense = Dropout(0.3)(dense)\n",
    "dense = Dense(4,activation='softmax')(dense)\n",
    "model = Model(inputs=[head_input,body_input], outputs=[dense])\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy',metrics=['acc'])\n",
    "plot_model(model, to_file=\"drive/My Drive/Stance_detection/feed_forward_lstm.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Conditional encoding model with summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 496
    },
    "colab_type": "code",
    "id": "KSrK_9BBF8h0",
    "outputId": "5cdc6089-4954-4aa4-a054-8e485c96dce5"
   },
   "outputs": [],
   "source": [
    "head_input = Input(shape=(max_head,), dtype='int32', name='head_input')\n",
    "body_input = Input(shape=(max_body,), dtype='int32', name='body_input')\n",
    "shared_embed = Embedding(len(word_index) + 1,EMBEDDING_DIM,weights=[embedding_matrix],trainable=False)\n",
    "head_embed = shared_embed(head_input)\n",
    "body_embed = shared_embed(body_input)\n",
    "\n",
    "body_lstm,state_h,state_c,x,y = Bidirectional(LSTM(100,dropout=0.2, recurrent_dropout=0.2, name='body_lstm',return_state=True))(body_embed)\n",
    "encoded_states = [state_h,state_c,x,y]\n",
    "head_lstm = Bidirectional(LSTM(100,dropout=0.2, recurrent_dropout=0.2, name='head_lstm'))(head_embed,initial_state=encoded_states)\n",
    "dot_layer = dot([head_lstm,body_lstm],axes = 1, normalize=True)\n",
    "conc = concatenate([head_lstm,body_lstm,dot_layer])\n",
    "dense = Dense(100,activation='relu')(conc)\n",
    "dense = Dropout(0.3)(dense)\n",
    "dense = Dense(100,activation='relu')(dense)\n",
    "dense = Dropout(0.3)(dense)\n",
    "dense = Dense(100,activation='relu')(dense)\n",
    "dense = Dropout(0.3)(dense)\n",
    "dense = Dense(4,activation='softmax')(dense)\n",
    "model = Model(inputs=[head_input,body_input], outputs=[dense])\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy',metrics=['acc'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model fit and save"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 853
    },
    "colab_type": "code",
    "id": "tFyKn_k6wc3m",
    "outputId": "c6c4f965-6a99-4e7a-c18f-4dfc15e92740"
   },
   "outputs": [],
   "source": [
    "history = model.fit([train_text,train_body],[y_train],epochs=20,validation_split=0.2 ,batch_size=128,verbose = True)\n",
    "model.save('drive/My Drive/Stance_detection/model/LSTM_feedforward.h5')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "eXkCb2TDx6Jp"
   },
   "source": [
    "# Model Evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 51
    },
    "colab_type": "code",
    "id": "6kG9xxv3wxuK",
    "outputId": "c8367517-919f-431b-c85a-394ba2e9fef3"
   },
   "outputs": [],
   "source": [
    "model.evaluate([test_text,test_body],[y_test])"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "machine_shape": "hm",
   "name": "news_stance_new.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
