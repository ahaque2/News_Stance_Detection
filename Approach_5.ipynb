{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 182
    },
    "colab_type": "code",
    "id": "QOQZA3-ciimC",
    "outputId": "de1cf473-3f8a-45bd-c697-62bc33036364"
   },
   "outputs": [],
   "source": [
    "import string\n",
    "import pandas as pd\n",
    "import gzip\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras.preprocessing.text import Tokenizer, text_to_word_sequence\n",
    "from keras.utils import to_categorical\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.layers import Activation, Conv2D, Input, Embedding, Reshape, MaxPool2D, Concatenate, Flatten, Dropout, Dense, Conv1D, MaxPooling1D, TimeDistributed\n",
    "from keras.layers import Dense, Flatten, LSTM, Conv1D, MaxPooling1D, Dropout, Activation, SpatialDropout1D, Layer, Embedding, Bidirectional, GRU, SpatialDropout2D\n",
    "from keras.layers import MaxPool1D\n",
    "from keras.models import Model\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.optimizers import Adam\n",
    "from keras.layers import concatenate,dot,add,subtract,multiply\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "import string\n",
    "import re\n",
    "from keras.models import Sequential\n",
    "from keras.metrics import top_k_categorical_accuracy\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from keras.callbacks import EarlyStopping\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "from keras import backend as K\n",
    "from keras import initializers\n",
    "from keras import regularizers, constraints\n",
    "from keras.utils import plot_model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Accessing Google drive and making sure gpu is active"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "jUnZ1zVeiyKX",
    "outputId": "615d6fe9-7193-46d1-ca49-510c5cb228b0"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "tf.test.gpu_device_name()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Global variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_SENTENCE_NUM = 100\n",
    "MAX_WORD_NUM = 100\n",
    "MAX_FEATURES = 200000 \n",
    "\n",
    "MAX_SENT_LENGTH = 200\n",
    "MAX_SENTS_body = 30\n",
    "MAX_SENTS_header = 2\n",
    "MAX_NB_WORDS = 200000\n",
    "EMBEDDING_DIM = 300\n",
    "VALIDATION_SPLIT = 0.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EiQ_vGqDj-3o"
   },
   "outputs": [],
   "source": [
    "path = \"drive/My Drive/Stance_detection/\"\n",
    "train_data_body = pd.read_csv(path + 'data/train_bodies.csv')\n",
    "train_data_stance = pd.read_csv(path + 'data/train_stances.csv')\n",
    "\n",
    "train_article_id = train_data_body['Body ID']\n",
    "train_stance_id = train_data_stance['Body ID']\n",
    "\n",
    "train_article_body = train_data_body['articleBody']\n",
    "train_labels = train_data_stance['Stance']\n",
    "train_headlines = train_data_stance['Headline']\n",
    "\n",
    "test_data_body = pd.read_csv(path + 'data/test_bodies.csv')\n",
    "test_data_stance = pd.read_csv(path + 'data/test_stances.csv')\n",
    "\n",
    "test_article_id = test_data_body['Body ID']\n",
    "test_stance_id = test_data_stance['Body ID']\n",
    "\n",
    "test_article_body = test_data_body['articleBody']\n",
    "test_labels = test_data_stance['Stance']\n",
    "test_headlines = test_data_stance['Headline']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QFRySTEEkW-i"
   },
   "outputs": [],
   "source": [
    "train_df = pd.merge(train_data_stance,train_data_body,on='Body ID',how='inner')\n",
    "test_df = pd.merge(test_data_stance,test_data_body,on='Body ID',how='inner')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "EqQEDiXrkZDp"
   },
   "outputs": [],
   "source": [
    "# Clean String\n",
    "def cleanString(review,stopWords):\n",
    "    \"\"\"\n",
    "    Cleans input string using set rules.\n",
    "    Cleaning rules:         Every word is lemmatized and lowercased. Stopwords and non alpha-numeric words are removed.\n",
    "                            Each sentence ends with a period.\n",
    "    Input:   review       - string(in sentence structure)\n",
    "             stopWords    - set of strings which should be removed from review\n",
    "    Output:  returnString - cleaned input string\n",
    "             idx_list     - list of lists, one list is equal to one sentence. In every list are the index\n",
    "                            of each word as they appeared in the non cleaned sentence\n",
    "                            e.g. nonCleaned = \"This is a test.\" -> cleaned = \"This test.\" -> cleaned_index = [[0,3]]\n",
    "    \"\"\"\n",
    "    # Init the Wordnet Lemmatizer\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    returnString = \"\"\n",
    "    sentence_token = sent_tokenize(review)\n",
    "    idx_list = []\n",
    "    for j in range(len(sentence_token)):\n",
    "        single_sentence = word_tokenize(sentence_token[j])\n",
    "        sentences_filtered = [(idx,lemmatizer.lemmatize(w.lower())) for idx,w in enumerate(single_sentence)\n",
    "                              if w.lower() not in stopWords and w.isalnum()]\n",
    "        idx_list.append([x[0] for x in sentences_filtered])\n",
    "        word_list = [x[1] for x in sentences_filtered]\n",
    "        returnString = returnString + ' '.join(word_list) + ' . '\n",
    "    return returnString, idx_list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Processing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "A8Wk8GbblFiz",
    "outputId": "ed978fe6-8b86-4d73-b737-729d16c53a50"
   },
   "outputs": [],
   "source": [
    "# training\n",
    "bodys = []\n",
    "labels = []\n",
    "texts = []\n",
    "headers = []\n",
    "\n",
    "# Tokenization\n",
    "# Word index\n",
    "\n",
    "\"\"\"\n",
    "Using the keras Tokenizer class a word index is built.\n",
    "The most 'MAX_FEATURES' used words are tokenized to a number.\n",
    "this dictionary is saved in word_index\n",
    "\"\"\"\n",
    "texts = []\n",
    "for i in range(train_df.shape[0]):\n",
    "    # Body\n",
    "    body = train_df['articleBody'].iloc[i]\n",
    "    body = ' '.join([word.strip(string.punctuation) for word in body.split() if word.strip(string.punctuation) is not \"\"])\n",
    "    body, _ = cleanString(body, stopwords.words(\"english\"))\n",
    "    texts.append(body)\n",
    "    sentences = sent_tokenize(body)\n",
    "    bodys.append(sentences)\n",
    "    \n",
    "    # Header\n",
    "    header = train_df['Headline'].iloc[i]\n",
    "    header = ' '.join([word.strip(string.punctuation) for word in header.split() if word.strip(string.punctuation) is not \"\"])\n",
    "    header, _ = cleanString(header, stopwords.words(\"english\"))\n",
    "    texts.append(header)\n",
    "    sentences = sent_tokenize(header)\n",
    "    headers.append(sentences)\n",
    "\n",
    "    #lables\n",
    "    labels.append(train_df['Stance'].iloc[i])\n",
    "\n",
    "# Testing\n",
    "test_bodys = []\n",
    "test_labels = []\n",
    "test_headers = []\n",
    "\n",
    "# Tokenization\n",
    "# Word index\n",
    "\n",
    "\"\"\"\n",
    "Using the keras Tokenizer class a word index is built.\n",
    "The most 'MAX_FEATURES' used words are tokenized to a number.\n",
    "this dictionary is saved in word_index\n",
    "\"\"\"\n",
    "for i in range(test_df.shape[0]):\n",
    "    # Body\n",
    "    body = test_df['articleBody'].iloc[i]\n",
    "    body = ' '.join([word.strip(string.punctuation) for word in body.split() if word.strip(string.punctuation) is not \"\"])\n",
    "    body, _ = cleanString(body, stopwords.words(\"english\"))\n",
    "    texts.append(body)\n",
    "    sentences = sent_tokenize(body)\n",
    "    test_bodys.append(sentences)\n",
    "    \n",
    "    # Header\n",
    "    header = test_df['Headline'].iloc[i]\n",
    "    header = ' '.join([word.strip(string.punctuation) for word in header.split() if word.strip(string.punctuation) is not \"\"])\n",
    "    header, _ = cleanString(header, stopwords.words(\"english\"))\n",
    "    texts.append(header)\n",
    "    sentences = sent_tokenize(header)\n",
    "    test_headers.append(sentences)\n",
    "\n",
    "    #lables\n",
    "    test_labels.append(test_df['Stance'].iloc[i])\n",
    "    \n",
    "tokenizer = keras.preprocessing.text.Tokenizer(num_words=MAX_NB_WORDS,lower=True, oov_token=None)\n",
    "tokenizer.fit_on_texts(texts)\n",
    "\n",
    "word_index = tokenizer.word_index\n",
    "print('Number of tokens: ' + str(len(word_index)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading and processing word embedding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 85
    },
    "colab_type": "code",
    "id": "mLBwR07zp7r_",
    "outputId": "3ac16de3-c8c0-48f6-ecf8-b014714fa98e"
   },
   "outputs": [],
   "source": [
    "# Word Embedding\n",
    "embeddings_index = dict()\n",
    "#f = open('drive/My Drive/Google_colab/ADBI_Text_Analysis/src/GloVe/pre-trained/glove/glove.6B.100d.txt')\n",
    "f = open(\"drive/My Drive/Google_colab/ADBI_Text_Analysis/src/GloVe/pre-trained/GoogleNews-vectors-negative300.txt\")\n",
    "for line in f:\n",
    "    values = line.split()\n",
    "    word = values[0]\n",
    "    coefs = np.asarray(values[1:], dtype='float32')\n",
    "    embeddings_index[word] = coefs\n",
    "f.close()\n",
    "print('Loaded %s word vectors.' % len(embeddings_index))\n",
    "\n",
    "\n",
    "EMBED_SIZE = 300\n",
    "\n",
    "min_wordCount = 2\n",
    "absent_words = 0\n",
    "small_words = 0\n",
    "#embedding_matrix = np.random.random((len(word_index) + 1, EMBEDDING_DIM))\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, EMBED_SIZE))\n",
    "word_counts = tokenizer.word_counts\n",
    "for word, i in word_index.items():\n",
    "    if word_counts[word] > min_wordCount:\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "        if embedding_vector is not None:\n",
    "            # words not found in embedding index will be all-zeros.\n",
    "            embedding_matrix[i] = embedding_vector\n",
    "        else:\n",
    "            absent_words += 1\n",
    "    else:\n",
    "        small_words += 1\n",
    "print('Total absent words are', absent_words, 'which is', \"%0.2f\" % (absent_words * 100 / len(word_index)),\n",
    "      '% of total words')\n",
    "print('Words with '+str(min_wordCount)+' or less mentions', small_words, 'which is', \"%0.2f\" % (small_words * 100 / len(word_index)),\n",
    "      '% of total words')\n",
    "print(str(len(word_index)-small_words-absent_words) + ' words to proceed.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Converting Data for desired format for hierarchical attention network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "RImEeoSxp8N1",
    "outputId": "93615e05-1097-473f-ecf7-8a5431df2ae1"
   },
   "outputs": [],
   "source": [
    "# preparing data\n",
    "classes = {'unrelated': 0 , 'agree':1, 'disagree':2, 'discuss':3}\n",
    "_labels = [classes[t] for t in labels]\n",
    "data_body = np.zeros((len(bodys), MAX_SENTS_body, MAX_SENT_LENGTH), dtype='int32')\n",
    "data_header = np.zeros((len(headers), MAX_SENTS_header, MAX_SENT_LENGTH), dtype='int32')\n",
    "\n",
    "headers\n",
    "for i, sentences in enumerate(bodys):\n",
    "    for j, sent in enumerate(sentences):\n",
    "        if j < MAX_SENTS_body:\n",
    "            wordTokens = text_to_word_sequence(sent)\n",
    "            k = 0\n",
    "            for _, word in enumerate(wordTokens):\n",
    "                if k < MAX_SENT_LENGTH and tokenizer.word_index[word] < MAX_NB_WORDS:\n",
    "                    data_body[i, j, k] = tokenizer.word_index[word]\n",
    "                    k = k + 1\n",
    "\n",
    "for i, sentences in enumerate(headers):\n",
    "    for j, sent in enumerate(sentences):\n",
    "        if j < MAX_SENTS_header:\n",
    "            wordTokens = text_to_word_sequence(sent)\n",
    "            k = 0\n",
    "            for _, word in enumerate(wordTokens):\n",
    "                if k < MAX_SENT_LENGTH and tokenizer.word_index[word] < MAX_NB_WORDS:\n",
    "                    data_header[i, j, k] = tokenizer.word_index[word]\n",
    "                    k = k + 1\n",
    "\n",
    "_labels = to_categorical(np.asarray(_labels))\n",
    "print('Shape of header tensor:', data_header.shape)\n",
    "print('Shape of bofy tensor:', data_body.shape)\n",
    "print('Shape of label tensor:', _labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "kXOKhrt7UBEk",
    "outputId": "aedec0be-bc6a-46ec-c1b9-81c94408dd55"
   },
   "outputs": [],
   "source": [
    "# preparing test data\n",
    "classes = {'agree':0,'discuss':1,'unrelated':2,'disagree':3}\n",
    "_test_labels = [classes[t] for t in test_labels]\n",
    "test_data_body = np.zeros((len(test_bodys), MAX_SENTS_body, MAX_SENT_LENGTH), dtype='int32')\n",
    "test_data_header = np.zeros((len(test_headers), MAX_SENTS_header, MAX_SENT_LENGTH), dtype='int32')\n",
    "\n",
    "headers\n",
    "for i, sentences in enumerate(test_bodys):\n",
    "    for j, sent in enumerate(sentences):\n",
    "        if j < MAX_SENTS_body:\n",
    "            wordTokens = text_to_word_sequence(sent)\n",
    "            k = 0\n",
    "            for _, word in enumerate(wordTokens):\n",
    "                if k < MAX_SENT_LENGTH and tokenizer.word_index[word] < MAX_NB_WORDS:\n",
    "                    test_data_body[i, j, k] = tokenizer.word_index[word]\n",
    "                    k = k + 1\n",
    "\n",
    "for i, sentences in enumerate(test_headers):\n",
    "    for j, sent in enumerate(sentences):\n",
    "        if j < MAX_SENTS_header:\n",
    "            wordTokens = text_to_word_sequence(sent)\n",
    "            k = 0\n",
    "            for _, word in enumerate(wordTokens):\n",
    "                if k < MAX_SENT_LENGTH and tokenizer.word_index[word] < MAX_NB_WORDS:\n",
    "                    test_data_header[i, j, k] = tokenizer.word_index[word]\n",
    "                    k = k + 1\n",
    "\n",
    "_test_labels = to_categorical(np.asarray(_test_labels))\n",
    "print('Shape of header tensor:', test_data_header.shape)\n",
    "print('Shape of bofy tensor:', test_data_body.shape)\n",
    "print('Shape of label tensor:', _test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Custom attention layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-v9Y3Gj3jDLb"
   },
   "outputs": [],
   "source": [
    "def dot_product(x, kernel):\n",
    "    \"\"\"\n",
    "    Wrapper for dot product operation, in order to be compatible with both\n",
    "    Theano and Tensorflow\n",
    "    Args:\n",
    "        x (): input\n",
    "        kernel (): weights\n",
    "    Returns:\n",
    "    \"\"\"\n",
    "    #print(\"dot: \", x.shape)\n",
    "    if K.backend() == 'tensorflow':\n",
    "        return K.squeeze(K.dot(x, K.expand_dims(kernel)), axis=-1)\n",
    "    else:\n",
    "        return K.dot(x, kernel)\n",
    "    \n",
    "\n",
    "class AttentionWithContext(Layer):\n",
    "    \"\"\"\n",
    "    Attention operation, with a context/query vector, for temporal data.\n",
    "    Supports Masking.\n",
    "    Follows the work of Yang et al. [https://www.cs.cmu.edu/~diyiy/docs/naacl16.pdf]\n",
    "    \"Hierarchical Attention Networks for Document Classification\"\n",
    "    by using a context vector to assist the attention\n",
    "    # Input shape\n",
    "        3D tensor with shape: `(samples, steps, features)`.\n",
    "    # Output shape\n",
    "        2D tensor with shape: `(samples, features)`.\n",
    "    How to use:\n",
    "    Just put it on top of an RNN Layer (GRU/LSTM/SimpleRNN) with return_sequences=True.\n",
    "    The dimensions are inferred based on the output shape of the RNN.\n",
    "    Note: The layer has been tested with Keras 2.0.6\n",
    "    Example:\n",
    "        model.add(LSTM(64, return_sequences=True))\n",
    "        model.add(AttentionWithContext())\n",
    "        # next add a Dense layer (for classification/regression) or whatever...\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, attention_dim=100,\n",
    "                 W_regularizer=None, u_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, u_constraint=None, b_constraint=None,\n",
    "                 bias=True, **kwargs):\n",
    "\n",
    "        self.supports_masking = True\n",
    "        self.init = initializers.get('glorot_uniform')\n",
    "\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.u_regularizer = regularizers.get(u_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.u_constraint = constraints.get(u_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        \n",
    "        self.attention_dim = attention_dim\n",
    "        \n",
    "        super(AttentionWithContext, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        #print(input_shape[-1])\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.W = self.add_weight((self.attention_dim, input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight((self.attention_dim,),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "\n",
    "        self.u = self.add_weight((self.attention_dim,),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_u'.format(self.name),\n",
    "                                 regularizer=self.u_regularizer,\n",
    "                                 constraint=self.u_constraint)\n",
    "\n",
    "        super(AttentionWithContext, self).build(input_shape)\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        # do not pass the mask to the next layers\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        \n",
    "        #print(\"here: \", x.shape)\n",
    "        #print(\"here: \", self.W.shape)\n",
    "        uit = dot_product(x, self.W)\n",
    "\n",
    "        if self.bias:\n",
    "            uit += self.b\n",
    "\n",
    "        uit = K.tanh(uit)\n",
    "        \n",
    "        ait = dot_product(uit, self.u)\n",
    "\n",
    "        a = K.exp(ait)\n",
    "\n",
    "        # apply mask after the exp. will be re-normalized next\n",
    "        if mask is not None:\n",
    "            # Cast the mask to floatX to avoid float64 upcasting in theano\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "\n",
    "        # in some cases especially in the early stages of training the sum may be almost zero\n",
    "        # and this results in NaN's. A workaround is to add a very small positive number Îµ to the sum.\n",
    "        # a /= K.cast(K.sum(a, axis=1, keepdims=True), K.floatx())\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "\n",
    "        a = K.expand_dims(a)\n",
    "        weighted_input = x * a\n",
    "        return K.sum(weighted_input, axis=1)\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return input_shape[0], input_shape[-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "uaI4JkgQkDrH"
   },
   "outputs": [],
   "source": [
    "head_input = Input(shape=(MAX_SENT_LENGTH,), dtype='int32', name='head_input')\n",
    "body_input = Input(shape=(MAX_SENT_LENGTH,), dtype='int32', name='body_input')\n",
    "embedding_layer = Embedding(len(word_index) + 1,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SENT_LENGTH,\n",
    "                            trainable=True,\n",
    "                            mask_zero=True)\n",
    "head_embed = embedding_layer(head_input)\n",
    "body_embed = embedding_layer(body_input)\n",
    "\n",
    "# Header encoding\n",
    "head_l_lstm = Bidirectional(GRU(100, return_sequences=True))(head_embed)\n",
    "head_l_lstm = SpatialDropout1D(0.5)(head_l_lstm)\n",
    "head_l_att = AttentionWithContext(100)(head_l_lstm)\n",
    "head_sentEncoder = Model(head_input, head_l_att)\n",
    "\n",
    "head_review_input = Input(shape=(MAX_SENTS_header, MAX_SENT_LENGTH), dtype='int32')\n",
    "review_encoder = TimeDistributed(head_sentEncoder)(head_review_input)\n",
    "head_l_lstm_sent = Bidirectional(GRU(100, return_sequences=True))(review_encoder)\n",
    "head_l_lstm_sent = SpatialDropout1D(0.5)(head_l_lstm_sent)\n",
    "head_l_att_sent = AttentionWithContext(100)(head_l_lstm_sent)\n",
    "head_dense = Dense(100,activation='relu')(head_l_att_sent)\n",
    "\n",
    "#Body encoding\n",
    "\n",
    "body_l_lstm = Bidirectional(GRU(100, return_sequences=True))(body_embed)\n",
    "body_l_lstm = SpatialDropout1D(0.5)(body_l_lstm)\n",
    "body_l_att = AttentionWithContext(100)(body_l_lstm)\n",
    "body_sentEncoder = Model(body_input, body_l_att)\n",
    "\n",
    "body_review_input = Input(shape=(MAX_SENTS_body, MAX_SENT_LENGTH), dtype='int32')\n",
    "review_encoder = TimeDistributed(body_sentEncoder)(body_review_input)\n",
    "body_l_lstm_sent = Bidirectional(GRU(100, return_sequences=True))(review_encoder)\n",
    "body_l_lstm_sent = SpatialDropout1D(0.5)(body_l_lstm_sent)\n",
    "body_l_att_sent = AttentionWithContext(100)(body_l_lstm_sent)\n",
    "body_dense = Dense(100,activation='relu')(body_l_att_sent)\n",
    "\n",
    "# Dot layer\n",
    "dot_layer = dot([head_dense,body_dense],axes = 1, normalize=True)\n",
    "conc = concatenate([head_dense,body_dense,dot_layer])\n",
    "dense = Dense(100,activation='relu')(conc)\n",
    "dense = Dropout(0.3)(dense)\n",
    "dense = Dense(4,activation='softmax')(dense)\n",
    "model = Model(inputs=[head_review_input,body_review_input], outputs=[dense])\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 547
    },
    "colab_type": "code",
    "id": "XzS77tXxk3Nr",
    "outputId": "03066db6-4bd5-443e-b6b6-77370449f9e5"
   },
   "outputs": [],
   "source": [
    "head_input = Input(shape=(MAX_SENT_LENGTH,), dtype='int32', name='head_input')\n",
    "body_input = Input(shape=(MAX_SENT_LENGTH,), dtype='int32', name='body_input')\n",
    "embedding_layer = Embedding(len(word_index) + 1,\n",
    "                            EMBEDDING_DIM,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=MAX_SENT_LENGTH,\n",
    "                            trainable=False,\n",
    "                            mask_zero=True)\n",
    "head_embed = embedding_layer(head_input)\n",
    "body_embed = embedding_layer(body_input)\n",
    "\n",
    "# Header encoding\n",
    "head_l_lstm = Bidirectional(LSTM(100, return_sequences=True))(head_embed)\n",
    "head_l_lstm = SpatialDropout1D(0.3)(head_l_lstm)\n",
    "head_l_att = AttentionWithContext(100)(head_l_lstm)\n",
    "head_sentEncoder = Model(head_input, head_l_att)\n",
    "\n",
    "head_review_input = Input(shape=(MAX_SENTS_header, MAX_SENT_LENGTH), dtype='int32')\n",
    "review_encoder = TimeDistributed(head_sentEncoder)(head_review_input)\n",
    "head_l_lstm_sent = Bidirectional(LSTM(100, return_sequences=True))(review_encoder)\n",
    "head_l_lstm_sent = SpatialDropout1D(0.3)(head_l_lstm_sent)\n",
    "head_l_att_sent = AttentionWithContext(100)(head_l_lstm_sent)\n",
    "#head_dense = Dense(100,activation='relu')(head_l_att_sent)\n",
    "\n",
    "#Body encoding\n",
    "\n",
    "body_l_lstm = Bidirectional(LSTM(100, return_sequences=True))(body_embed)\n",
    "body_l_lstm = SpatialDropout1D(0.3)(body_l_lstm)\n",
    "body_l_att = AttentionWithContext(100)(body_l_lstm)\n",
    "body_sentEncoder = Model(body_input, body_l_att)\n",
    "\n",
    "body_review_input = Input(shape=(MAX_SENTS_body, MAX_SENT_LENGTH), dtype='int32')\n",
    "review_encoder = TimeDistributed(body_sentEncoder)(body_review_input)\n",
    "body_l_lstm_sent = Bidirectional(LSTM(100, return_sequences=True))(review_encoder)\n",
    "body_l_lstm_sent = SpatialDropout1D(0.3)(body_l_lstm_sent)\n",
    "body_l_att_sent = AttentionWithContext(100)(body_l_lstm_sent)\n",
    "#body_dense = Dense(100,activation='relu')(body_l_att_sent)\n",
    "\n",
    "# Dot layer\n",
    "dot_layer = dot([head_l_att_sent,body_l_att_sent],axes = 1, normalize=True)\n",
    "conc = concatenate([head_l_att_sent,body_l_att_sent,dot_layer])\n",
    "dense = Dense(100,activation='relu')(conc)\n",
    "dense = Dropout(0.3)(dense)\n",
    "dense = Dense(4,activation='softmax')(dense)\n",
    "model = Model(inputs=[head_review_input,body_review_input], outputs=[dense])\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model fitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 326
    },
    "colab_type": "code",
    "id": "hIKz25yaYiUJ",
    "outputId": "b2f17e12-2794-4391-eea8-d7e5392ff6ea"
   },
   "outputs": [],
   "source": [
    "history = model.fit([data_header,data_body], _labels, validation_split=0.2, nb_epoch=10, batch_size=128)\n",
    "model.save('drive/My Drive/Stance_detection/model/HAN_simple_lstm.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "j2qHjpYzYi9v"
   },
   "source": [
    "# Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pcda_7YWh3Tn"
   },
   "outputs": [],
   "source": [
    "model.evaluate([test_data_header,test_data_body],[_test_labels])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ud7WCeygeZbS"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "machine_shape": "hm",
   "name": "stance_detection.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
