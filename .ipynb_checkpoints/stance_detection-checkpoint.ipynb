{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [],
   "source": [
    "import corenlp\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import sys\n",
    "from gensim.models import KeyedVectors\n",
    "import os\n",
    "from cleantext import clean\n",
    "import pprint\n",
    "import sys\n",
    "import itertools\n",
    "import nltk\n",
    "import re\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from string import punctuation\n",
    "from autocorrect import spell\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "from nltk.tokenize import sent_tokenize\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_body = pd.read_csv('data/train_bodies.csv')\n",
    "train_data_stance = pd.read_csv('data/train_stances.csv')\n",
    "\n",
    "train_article_id = train_data_body['Body ID']\n",
    "train_stance_id = train_data_stance['Body ID']\n",
    "\n",
    "train_article_body = train_data_body['articleBody']\n",
    "train_labels = train_data_stance['Stance']\n",
    "train_headlines = train_data_stance['Headline']\n",
    "\n",
    "test_data_body = pd.read_csv('data/competition_test_bodies.csv')\n",
    "test_data_stance = pd.read_csv('data/competition_test_stances.csv')\n",
    "\n",
    "test_article_id = test_data_body['Body ID']\n",
    "test_stance_id = test_data_stance['Body ID']\n",
    "\n",
    "test_article_body = test_data_body['articleBody']\n",
    "test_labels = test_data_stance['Stance']\n",
    "test_headlines = test_data_stance['Headline']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_vectorizor(model, tokenized_sent, dim):\n",
    "    \n",
    "    return np.array([\n",
    "            np.mean([model[w] for w in words if w in model.vocab]\n",
    "                    or [np.zeros(dim)], axis=0)\n",
    "            for words in tokenized_sent\n",
    "        ])\n",
    "model = KeyedVectors.load_word2vec_format('../word_embedings/GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_tokenized_body_para1(text, dim=5):\n",
    "    \n",
    "    tokenized_sent = sent_tokenize(text)\n",
    "    body_text = tokenized_sent[0:dim]\n",
    "    bt = []\n",
    "    [bt.extend(b.split('.')[:-1]) for b in body_text]\n",
    "    return bt\n",
    "\n",
    "def get_ner_tags(news_articles):\n",
    "    \n",
    "    #ner_tags = dict()\n",
    "    ner_tags = []\n",
    "    os.environ['CORENLP_HOME'] = '/home/ahaque2/project/virtual_environment_1/stanfordNLP'\n",
    "    with corenlp.CoreNLPClient(annotators=\"tokenize ssplit pos lemma ner depparse\".split()) as client:\n",
    "\n",
    "        for sent in news_articles:\n",
    "            ann = client.annotate(sent)\n",
    "            sentence = ann.sentence\n",
    "            tokens = sentence[0].token\n",
    "            for tok in tokens:\n",
    "                if(tok.ner != 'O'):\n",
    "                    ner_tags.append(tok.word)\n",
    "                    '''\n",
    "                    if tok.ner in ner_tags:\n",
    "                        ner_tags[tok.ner].append(tok.word)\n",
    "                    else:\n",
    "                        ner_tags[tok.ner] = []\n",
    "                        ner_tags[tok.ner].append(tok.word)\n",
    "                    '''\n",
    "        #print(i, end=\" \")\n",
    "    #print(ner_tags)\n",
    "    return ner_tags\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Code for Text preprocessing\n",
    "def autospell(text):\n",
    "    \"\"\"\n",
    "    correct the spelling of the word.\n",
    "    \"\"\"\n",
    "    spells = [spell(w) for w in (nltk.word_tokenize(text))]\n",
    "    return \" \".join(spells)\n",
    "\n",
    "def to_lower(text):\n",
    "    \"\"\"\n",
    "    :param text:\n",
    "    :return:\n",
    "        Converted text to lower case as in, converting \"Hello\" to \"hello\" or \"HELLO\" to \"hello\".\n",
    "    \"\"\"\n",
    "    return text.lower()\n",
    "\n",
    "def remove_numbers(text):\n",
    "    \"\"\"\n",
    "    take string input and return a clean text without numbers.\n",
    "    Use regex to discard the numbers.\n",
    "    \"\"\"\n",
    "    output = ''.join(c for c in text if not c.isdigit())\n",
    "    return output\n",
    "\n",
    "def remove_punct(text):\n",
    "    \"\"\"\n",
    "    take string input and clean string without punctuations.\n",
    "    use regex to remove the punctuations.\n",
    "    \"\"\"\n",
    "    return ''.join(c for c in text if c not in punctuation)\n",
    "\n",
    "def remove_Tags(text):\n",
    "    \"\"\"\n",
    "    take string input and clean string without tags.\n",
    "    use regex to remove the html tags.\n",
    "    \"\"\"\n",
    "    cleaned_text = re.sub('<[^<]+?>', '', text)\n",
    "    return cleaned_text\n",
    "\n",
    "def sentence_tokenize(text):\n",
    "    \"\"\"\n",
    "    take string input and return list of sentences.\n",
    "    use nltk.sent_tokenize() to split the sentences.\n",
    "    \"\"\"\n",
    "    sent_list = []\n",
    "    for w in nltk.sent_tokenize(text):\n",
    "        sent_list.append(w)\n",
    "    return sent_list\n",
    "\n",
    "def word_tokenize(text):\n",
    "    \"\"\"\n",
    "    :param text:\n",
    "    :return: list of words\n",
    "    \"\"\"\n",
    "    return [w for sent in nltk.sent_tokenize(text) for w in nltk.word_tokenize(sent)]\n",
    "\n",
    "def remove_stopwords(sentence):\n",
    "    \"\"\"\n",
    "    removes all the stop words like \"is,the,a, etc.\"\n",
    "    \"\"\"\n",
    "    stop_words = stopwords.words('english')\n",
    "    return ' '.join([w for w in nltk.word_tokenize(sentence) if not w in stop_words])\n",
    "\n",
    "def stem(text):\n",
    "    \"\"\"\n",
    "    :param word_tokens:\n",
    "    :return: list of words\n",
    "    \"\"\"\n",
    "    \n",
    "    snowball_stemmer = SnowballStemmer('english')\n",
    "    stemmed_word = [snowball_stemmer.stem(word) for sent in nltk.sent_tokenize(text)for word in nltk.word_tokenize(sent)]\n",
    "    return \" \".join(stemmed_word)\n",
    "\n",
    "def lemmatize(text):\n",
    "    \n",
    "    wordnet_lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_word = [wordnet_lemmatizer.lemmatize(word)for sent in nltk.sent_tokenize(text)for word in nltk.word_tokenize(sent)]\n",
    "    return \" \".join(lemmatized_word)\n",
    "\n",
    "\n",
    "def preprocess(text):\n",
    "\n",
    "    lower_text = to_lower(text)\n",
    "    sentence_tokens = sentence_tokenize(lower_text)\n",
    "    word_list = []\n",
    "    for each_sent in sentence_tokens:\n",
    "        lemmatizzed_sent = lemmatize(each_sent)\n",
    "        clean_text = remove_numbers(lemmatizzed_sent)\n",
    "        clean_text = remove_punct(clean_text)\n",
    "        clean_text = remove_Tags(clean_text)\n",
    "        clean_text = remove_stopwords(clean_text)\n",
    "        word_tokens = word_tokenize(clean_text)\n",
    "        for i in word_tokens:\n",
    "            word_list.append(i)\n",
    "    return word_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_features(article_id, article_body, stance_id):\n",
    "    df = pd.DataFrame(columns = ['stance_id', 'similarity', 'label'])\n",
    "    for bid, txt in zip(article_id, article_body):\n",
    "        index = np.where(stance_id == bid)[0]\n",
    "        article = get_tokenized_body_para1(txt)\n",
    "        lab = labels.iloc[index]\n",
    "        heads = headlines.iloc[index]\n",
    "        heads_tokenized = [preprocess(h) for h in heads]\n",
    "\n",
    "        article = list(itertools.chain.from_iterable([preprocess(a) for a in article]))\n",
    "        article_vec = mean_vectorizor(model, [article] , 300)\n",
    "\n",
    "        head_vec = mean_vectorizor(model, heads_tokenized, 300)\n",
    "    \n",
    "        similarity = []\n",
    "        from scipy import spatial\n",
    "        for h in head_vec:\n",
    "            similarity.append(1 - spatial.distance.cosine(h, article_vec))\n",
    "        \n",
    "        df2 = pd.DataFrame(columns = ['stance_id', 'similarity', 'label'])\n",
    "        df2['stance_id'] = index\n",
    "        df2['similarity'] = similarity\n",
    "        df2['label'] = np.array(lab)\n",
    "\n",
    "        #print(df2.shape)\n",
    "        df = df.append(df2, ignore_index = True)\n",
    "        #print(df.shape)\n",
    "        #sys.exit()\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ahaque2/project/virtual_environment_1/lib/python3.5/site-packages/scipy/spatial/distance.py:720: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  dist = 1.0 - uv / np.sqrt(uu * vv)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(49972, 3) (25413, 3)\n"
     ]
    }
   ],
   "source": [
    "df_train = get_features(train_article_id, train_article_body, train_stance_id)\n",
    "df_test = get_features(test_article_id, test_article_body, test_stance_id)\n",
    "print(df_train.shape, df_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_Xy(df):\n",
    "    \n",
    "    X = df['similarity']\n",
    "    y = df['label']\n",
    "    #print(X.shape, y.shape)\n",
    "    i = np.where(X.isna() == True)[0]\n",
    "    df = df.drop(i)\n",
    "    X = df['similarity']\n",
    "    y = df['label']\n",
    "    \n",
    "    '''\n",
    "    y = y.replace('unrelated', 1)\n",
    "    y = y.replace('agree', 0)\n",
    "    y = y.replace('discuss', 0)\n",
    "    y = y.replace('disagree', 0)\n",
    "    '''\n",
    "    \n",
    "    y = np.array(y).reshape(-1,1)\n",
    "    X = np.array(X).reshape(-1,1)\n",
    "    return X, y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'disagree': 446, 'agree': 1840, 'discuss': 4528, 'unrelated': 18537}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ahaque2/project/virtual_environment_1/lib/python3.5/site-packages/ipykernel_launcher.py:14: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  \n"
     ]
    }
   ],
   "source": [
    "X_train, y_train = get_Xy(df_train)\n",
    "X_test, y_test = get_Xy(df_test)\n",
    "unique, counts = np.unique(y_test, return_counts=True)\n",
    "print(dict(zip(unique, counts)))\n",
    "\n",
    "from sklearn import neighbors\n",
    "from sklearn.naive_bayes import GaussianNB, MultinomialNB\n",
    "from sklearn import tree\n",
    "clf = tree.DecisionTreeClassifier()\n",
    "clf = MultinomialNB()\n",
    "clf = neighbors.KNeighborsClassifier(9)\n",
    "\n",
    "#clf = LinearSVC(random_state=0, tol=1e-5)\n",
    "clf.fit(X_train, y_train)\n",
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'disagree': 446, 'agree': 1840, 'discuss': 4528, 'unrelated': 18537}\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "       agree       0.00      0.04      0.01       165\n",
      "    disagree       0.00      0.00      0.00         5\n",
      "     discuss       0.04      0.21      0.06       765\n",
      "   unrelated       0.96      0.73      0.83     24416\n",
      "\n",
      "    accuracy                           0.71     25351\n",
      "   macro avg       0.25      0.25      0.22     25351\n",
      "weighted avg       0.93      0.71      0.80     25351\n",
      "\n",
      "[[    7     2    40   116]\n",
      " [    0     0     1     4]\n",
      " [   51     9   160   545]\n",
      " [ 1782   435  4327 17872]]\n"
     ]
    }
   ],
   "source": [
    "unique, counts = np.unique(y_test, return_counts=True)\n",
    "print(dict(zip(unique, counts)))\n",
    "print(classification_report(y_pred, y_test))\n",
    "print(confusion_matrix(y_pred, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
